{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traditional_extractive_summerization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSEjwRhj9uls1SojYq13Dh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmoj1428/traditional-extractive-summerization-jacard/blob/main/traditional_extractive_summerization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aajDJa_-67bG",
        "outputId": "862ddc39-16dc-482f-f8c2-15f6e74fc8ec"
      },
      "source": [
        "!pip install hazm\n",
        "!pip install rouge\n",
        "from  __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from xml.dom import minidom\n",
        "from rouge import Rouge\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.5MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 10.2MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154154 sha256=2cbf0a29ffc0d8523815cbbee522da7027c2f047126956685ccb7368952f328d\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp37-none-any.whl size=1394471 sha256=69c484e55c624ded4f75ad2a3baf6a1b9f5721c4429453b452eadd75cd0cf51a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBj8OIIpwn-4",
        "outputId": "c0a5a9f6-e05b-4ba5-a3b2-409d93edec65"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jQL6U9l8b-u"
      },
      "source": [
        "# initialization\n",
        "class jacardSentSumm(object):\n",
        "  def __init__(self, text, fileName, summerizeLen = 10):\n",
        "    normalizer = Normalizer()\n",
        "    normalizeText = normalizer.normalize(text)\n",
        "    self.sentToken = sent_tokenize(normalizeText)\n",
        "    self.wordToken = word_tokenize(normalizeText)\n",
        "    self.lenPointDict = {}\n",
        "    self.summerizeLen = summerizeLen\n",
        "    self.fileName = fileName\n",
        "    self.text = \"\"\n",
        "    self.max_sent_length()\n",
        "\n",
        "  def max_sent_length(self):\n",
        "    for i in self.sentToken:\n",
        "      self.sentWords = word_tokenize(i)\n",
        "      self.lenPointDict[i] = len(self.sentWords)\n",
        "    maxLen = max(self.lenPointDict.values())\n",
        "    return self.index(maxLen)\n",
        "\n",
        "  # length index and position index and jacard index\n",
        "  def index(self, maxLen):\n",
        "    pointDict = {}\n",
        "    for i in self.sentToken:\n",
        "      self.mainSent = word_tokenize(i)\n",
        "      self.point = []\n",
        "      for j in self.sentToken:\n",
        "        if (i != j):\n",
        "          self.compSent = word_tokenize(j)\n",
        "          self.a = set(self.mainSent)&set(self.compSent)\n",
        "          self.comSent = sorted(self.a, key = lambda k : self.mainSent.index(k))\n",
        "          self.point.append(len(self.comSent) / ((len(self.mainSent) + len(self.compSent)-len(self.comSent))))\n",
        "      pointDict[i] = [0.15 * (len(self.mainSent) / maxLen), 0.3 * (1/(self.sentToken.index(i)+1)), 0.55 * (sum(self.point) / len(self.point))]\n",
        "    return self.calculate_final_point(pointDict)\n",
        "\n",
        "  # calculate final point\n",
        "  def calculate_final_point(self, pointDict):\n",
        "    finalPoint = {}\n",
        "    for i in pointDict:\n",
        "      point = sum(pointDict[i])\n",
        "      finalPoint[i] = point\n",
        "    return self.choose_sent(finalPoint)\n",
        "\n",
        "  # choose sentence\n",
        "  def choose_sent(self, finalPoint):\n",
        "    position = {}\n",
        "    self.text = \"\"\n",
        "    for i in range(self.summerizeLen):\n",
        "      self.finalSent = max(finalPoint.values())\n",
        "      self.keys = [k for k, v in finalPoint.items() if v == self.finalSent]\n",
        "      position[self.keys[0]] = self.sentToken.index(self.keys[0])\n",
        "      finalPoint.pop(self.keys[0])\n",
        "    for i in range(self.summerizeLen):\n",
        "      sent = min(position.values())\n",
        "      key = [i for i, j in position.items() if j == sent]\n",
        "      self.text += key[0]\n",
        "      position.pop(key[0])\n",
        "    f = open('/content/drive/My Drive/NLP/' + self.fileName[:-1] + '.txt', \"a\")\n",
        "    f.write(self.text)\n",
        "    f.close()\n",
        "\n",
        "  def rouge_point(self, mainText, compText):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(mainText, compText)\n",
        "    # print(scores)\n",
        "    file = open('/content/drive/My Drive/NLP/rougeResult.txt', \"a\")\n",
        "    file.write(self.fileName[:-1] + ': ' + str(scores) + '\\n')\n",
        "    file.close()\n",
        "    return scores\n",
        "\n",
        "  def rouge_average(self, pointResult):\n",
        "    rouge1f = rouge1p = rouge1r = rouge2f = rouge2p = rouge2r = rougelf = rougelp = rougelr = 0\n",
        "    rougeAverageResult = {}\n",
        "    resultLen = len(pointResult)\n",
        "    for i in pointResult:\n",
        "      rouge1Val = i[0].get('rouge-1')\n",
        "      rouge1f += rouge1Val.get('f')\n",
        "      rouge1p += rouge1Val.get('p')\n",
        "      rouge1r += rouge1Val.get('r')\n",
        "\n",
        "      rouge2Val = i[0].get('rouge-2')\n",
        "      rouge2f += rouge2Val.get('f')\n",
        "      rouge2p += rouge2Val.get('p')\n",
        "      rouge2r += rouge2Val.get('r')\n",
        "\n",
        "      rougelVal = i[0].get('rouge-l')\n",
        "      rougelf += rougelVal.get('f')\n",
        "      rougelp += rougelVal.get('p')\n",
        "      rougelr += rougelVal.get('r')\n",
        "    rougeAverageResult['rouge-1'] = {'f': rouge1f / resultLen, 'p': rouge1p / resultLen, 'r': rouge1r / resultLen}\n",
        "    rougeAverageResult['rouge-2'] = {'f': rouge2f / resultLen, 'p': rouge2p / resultLen, 'r': rouge2r / resultLen}\n",
        "    rougeAverageResult['rouge-l'] = {'f': rougelf / resultLen, 'p': rougelp / resultLen, 'r': rougelr / resultLen}\n",
        "\n",
        "    file = open('/content/drive/My Drive/NLP/rougeAvgResult.txt', \"a\")\n",
        "    file.write(str([rougeAverageResult]))\n",
        "    file.close()\n",
        "    return [rougeAverageResult]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-5dLilHjTc1"
      },
      "source": [
        "def set_path(line):\n",
        "  path = '/content/drive/My Drive/NLP/summDataSet/1/'+ line\n",
        "  if Path(path + '.E.S.A.F.C' + '.xml').is_file():\n",
        "    path1 = '/content/drive/My Drive/NLP/summDataSet/1/'+ line + '.E.S.A.F.C' + '.xml'\n",
        "  elif Path(path + '.E.S.c.m.c' + '.xml').is_file():\n",
        "    path1 = '/content/drive/My Drive/NLP/summDataSet/1/'+ line + '.E.S.c.m.c' + '.xml'\n",
        "  else:\n",
        "    path1 = '/content/drive/My Drive/NLP/summDataSet/1/'+ line + '.E.S.e.m.c' + '.xml'\n",
        "  try:\n",
        "    with open(path1, 'r') as f:\n",
        "      data = f.read()\n",
        "  except:\n",
        "    print(\"File not accessible!\")\n",
        "    data = \"\"\n",
        "  return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7VN891tuhPc",
        "outputId": "a1a6bcbf-b013-4bdd-b26f-146bffd8f768"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  dataPath = '/content/drive/My Drive/NLP/Index.txt'\n",
        "  summerizeLen = input(\"Enter summrization length:\")\n",
        "  rougeResult = []\n",
        "  with open(dataPath, 'r') as data:\n",
        "    lines = data.readlines()\n",
        "    lastLine = lines[-1]\n",
        "    for line in lines:\n",
        "      if(line != lastLine):\n",
        "        mydoc = minidom.parse('/content/drive/My Drive/NLP/dataSet/'+ line[:-1] + '.xml')\n",
        "        data = set_path(line[:-1])\n",
        "      else:\n",
        "        mydoc = minidom.parse('/content/drive/My Drive/NLP/dataSet/' + line + '.xml')\n",
        "        data = set_path(line)\n",
        "      items = mydoc.getElementsByTagName('TITLE')\n",
        "      items1 = mydoc.getElementsByTagName('SUMMARY')\n",
        "      items2 = mydoc.getElementsByTagName('TEXT')\n",
        "      if (len(items1[0].childNodes) != 0):\n",
        "        sentSummerize = jacardSentSumm(items[0].childNodes[0].data + '.' + items1[0].childNodes[0].data + ' .' + items2[0].childNodes[0].data, line, int(summerizeLen))\n",
        "      else:\n",
        "        sentSummerize = jacardSentSumm(items[0].childNodes[0].data + '.' + items2[0].childNodes[0].data, line, int(summerizeLen))\n",
        "      if (len(data) != 0):\n",
        "        rougeData = sentSummerize.rouge_point(data, sentSummerize.text)\n",
        "        rougeResult.append(rougeData)\n",
        "    averageResult = sentSummerize.rouge_average(rougeResult)\n",
        "    print(averageResult)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter summrization length:5\n",
            "File not accessible!\n",
            "[{'rouge-1': {'f': 0.4149564608117956, 'p': 0.5083667278188627, 'r': 0.3771572091588319}, 'rouge-2': {'f': 0.24824788914904122, 'p': 0.31603580595321623, 'r': 0.21686245220052566}, 'rouge-l': {'f': 0.3842722562555461, 'p': 0.4404523305555738, 'r': 0.3579917774657836}}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}